{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2515a1c-c4f9-4890-aa6d-641bf419cea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49835da8-c4d0-461a-a2c8-3c0959118bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.Timestamp\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.time._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.sql.Timestamp\n",
    "import java.time._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df9de1d-1f63-41b1-999b-69614c723fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "23/12/27 12:47:56 INFO SparkContext: Running Spark version 3.5.0\n",
      "23/12/27 12:47:56 INFO SparkContext: OS info Linux, 6.2.0-39-generic, amd64\n",
      "23/12/27 12:47:56 INFO SparkContext: Java version 11.0.21\n",
      "23/12/27 12:47:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/27 12:47:56 INFO ResourceUtils: ==============================================================\n",
      "23/12/27 12:47:56 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/12/27 12:47:56 INFO ResourceUtils: ==============================================================\n",
      "23/12/27 12:47:56 INFO SparkContext: Submitted application: Hello Spark\n",
      "23/12/27 12:47:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/12/27 12:47:56 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/12/27 12:47:56 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/12/27 12:47:56 INFO SecurityManager: Changing view acls to: vadim\n",
      "23/12/27 12:47:56 INFO SecurityManager: Changing modify acls to: vadim\n",
      "23/12/27 12:47:56 INFO SecurityManager: Changing view acls groups to: \n",
      "23/12/27 12:47:56 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/12/27 12:47:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: vadim; groups with view permissions: EMPTY; users with modify permissions: vadim; groups with modify permissions: EMPTY\n",
      "23/12/27 12:47:57 INFO Utils: Successfully started service 'sparkDriver' on port 45087.\n",
      "23/12/27 12:47:57 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/12/27 12:47:57 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/12/27 12:47:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/12/27 12:47:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/12/27 12:47:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/12/27 12:47:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d5f8c5cc-c7b4-4703-981a-53b50d100527\n",
      "23/12/27 12:47:57 INFO MemoryStore: MemoryStore started with capacity 4.5 GiB\n",
      "23/12/27 12:47:57 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/12/27 12:47:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "23/12/27 12:47:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/12/27 12:47:57 INFO Executor: Starting executor ID driver on host ubuntu\n",
      "23/12/27 12:47:57 INFO Executor: OS info Linux, 6.2.0-39-generic, amd64\n",
      "23/12/27 12:47:57 INFO Executor: Java version 11.0.21\n",
      "23/12/27 12:47:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/12/27 12:47:57 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@43763d83 for default.\n",
      "23/12/27 12:47:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45523.\n",
      "23/12/27 12:47:57 INFO NettyBlockTransferService: Server created on ubuntu:45523\n",
      "23/12/27 12:47:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/12/27 12:47:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ubuntu, 45523, None)\n",
      "23/12/27 12:47:57 INFO BlockManagerMasterEndpoint: Registering block manager ubuntu:45523 with 4.5 GiB RAM, BlockManagerId(driver, ubuntu, 45523, None)\n",
      "23/12/27 12:47:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ubuntu, 45523, None)\n",
      "23/12/27 12:47:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ubuntu, 45523, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3409c728\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"Hello Spark\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c101c3-900c-4a51-8f5f-bfc0ce535ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.version == 3.5.0\n"
     ]
    }
   ],
   "source": [
    "println(s\"spark.version == ${spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f60a79-f610-4973-a8e1-391f53ff22ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/27 12:47:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/12/27 12:47:59 INFO SharedState: Warehouse path is 'file:/home/vadim/workspace/Spark/spark-warehouse'.\n",
      "23/12/27 12:48:00 INFO CodeGenerator: Code generated in 200.824434 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [a: bigint, b: double ... 3 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = List(\n",
    "      (1L, 2.0, \"string1\", LocalDate.of(2000, 1, 1), Timestamp.valueOf(\"2023-01-01 12:00:00\")),\n",
    "      (2L, 3.0, \"string2\", LocalDate.of(2000, 2, 1), Timestamp.valueOf(\"2023-02-01 12:00:00\")),\n",
    "      (3L, 4.0, \"string3\", LocalDate.of(2000, 3, 1), Timestamp.valueOf(\"2023-03-01 12:00:00\"))\n",
    "    ).toDF(\"a\", \"b\", \"c\", \"d\", \"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a3e31b-2095-47dd-8528-1e2995755c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = false)\n",
      " |-- b: double (nullable = false)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11b4f0e8-4a87-42f5-bd1f-fbce9e438b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/27 12:48:02 INFO CodeGenerator: Code generated in 10.756904 ms\n",
      "23/12/27 12:48:02 INFO CodeGenerator: Code generated in 16.709099 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2023-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2023-02-01 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2023-03-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17457ede-310a-4b9f-b7f4-410b9a8c40cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/27 12:48:03 INFO CodeGenerator: Code generated in 8.302357 ms\n",
      "23/12/27 12:48:03 INFO CodeGenerator: Code generated in 10.425669 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  a|  b|      c|         d|                  e|upper_c|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  1|2.0|string1|2000-01-01|2023-01-01 12:00:00|STRING1|\n",
      "|  2|3.0|string2|2000-02-01|2023-02-01 12:00:00|STRING2|\n",
      "|  3|4.0|string3|2000-03-01|2023-03-01 12:00:00|STRING3|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"upper_c\", upper($\"c\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e5ae9ac-a8f6-4114-8833-d3a958aab518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/27 12:48:03 INFO CodeGenerator: Code generated in 7.107823 ms\n",
      "23/12/27 12:48:03 INFO CodeGenerator: Code generated in 7.9066 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      c|\n",
      "+-------+\n",
      "|string1|\n",
      "|string2|\n",
      "|string3|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"c\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cbc3349-1f2e-4652-88fb-94d7f8cc1b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/27 12:48:03 INFO CodeGenerator: Code generated in 4.889298 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2023-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter($\"a\" === 1L).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
